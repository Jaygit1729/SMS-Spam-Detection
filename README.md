ğŸ“± SMS Spam Classifier

ğŸš€ About this Project

This project involves building an SMS Spam Classifier using Natural Language Processing (NLP) and Machine Learning techniques. The goal is to accurately distinguish between spam and non-spam messages, achieving high precision and recall.

ğŸ“Š About the Dataset

The SMS Spam Collection is a dataset containing 5,574 English messages labeled as either ham (legitimate) or spam. Each line has:

v1: Label (ham or spam)

v2: Raw message text

This dataset was gathered from publicly available sources for research purposes.

ğŸ› ï¸ Methodology

1ï¸âƒ£ Data Ingestion and Cleaning

Dataset: SMS Spam Collection from Kaggle.

Contains 5,572 messages with 5 columns.

Removed: Unnecessary columns (Unnamed: 2, Unnamed: 3, Unnamed: 4).

Duplicates: Dropped 403 duplicates.

Null values: None found, ensuring data integrity.

2ï¸âƒ£ Feature Engineering and Analysis

New Features Created:

ğŸ“Œ Number of characters per message

ğŸ“Œ Number of words per message

ğŸ“Œ Number of sentences per message

- Statistical analysis provided insights into message length distributions and patterns.

3ï¸âƒ£ Exploratory Data Analysis (EDA)

ğŸ“Œ Target category counts:

87.37% 'ham' messages

12.63% 'spam' messages

ğŸ“Œ KDE Plot: Revealed skewness in message length and sentence count between categories.

ğŸ“Œ Heatmap: Correlation matrix to uncover feature relationships.

4ï¸âƒ£ Data Preprocessing

 Applied text preprocessing techniques:

ğŸ”¹ Lowercasing

ğŸ”¹ Tokenization

ğŸ”¹ Special characters removal

ğŸ”¹ Stopword removal

ğŸ”¹ Stemming

- Transformed target labels: 'ham' â†’ 0, 'spam' â†’ 1
- Generated word frequency scatter plots for spam vs ham messages.

5ï¸âƒ£ Text Vectorization (Count Vectorizer)

Count Vectorizer: Converted text into numerical format.

Generated: 6,708 features from the messages.

6ï¸âƒ£ Model Building and Evaluation

- Split data: 80:20 ratio (training : test)

- Models built:

Naive Bayes (base model)

Two additional models for performance comparison

ğŸ… Naive Bayes Classifier emerged as the best performer with balanced F1-Score & Precision.

âœ”ï¸ Saved: Final model and vectorizer for future use.

7ï¸âƒ£ Model Optimization and Saving

Feature selection: Chi-square test

Hyperparameter tuning: RandomizedSearchCV

Final evaluation: Performance checked on test set

ğŸ’¾ Saved:

Optimized Naive Bayes model (via joblib)

Selected feature set

8ï¸âƒ£ Streamlit Web App

âœ¨ Built an interactive Streamlit web application for real-time SMS spam classification. Users can input messages and get instant classification results.

ğŸ”— [Live Demo Link] - https://sms-spam-detection-ml.streamlit.app/

ğŸ‰ Conclusion

The developed SMS spam classifier:

âœ… High F1 Score: Differentiates spam vs ham messages effectively.

âœ… Best Model: Naive Bayes proved to be the most efficient performer.

âœ… Handles large features: Each word is a feature â€” thousands of features handled smoothly.

âœ… Resilient to irrelevant data: Performance remains strong despite noise.

âœ… Fast training & prediction times: Even with large datasets.

âš¡ Why Naive Bayes?

Simplicity: Works right out of the box.

Minimal tuning needed: Rarely requires hyperparameter adjustments.

Avoids overfitting: Performs consistently.

Handles high-dimensional data: Perfect for text classification.

ğŸš€ Happy Classifying!

